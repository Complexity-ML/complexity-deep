# MMLU Pure SFT - 100% MMLU for benchmark optimization
# 10 epochs, focused training on MMLU auxiliary_train only

model:
  checkpoint: "./checkpoints-conv-sft/checkpoint_epoch6.pt"
  tokenizer: "./checkpoints-conv-sft"
  output: "./checkpoints-mmlu-pure"

training:
  epochs: 30
  batch_size: 8
  gradient_accumulation: 4
  learning_rate: 1e-5
  weight_decay: 0.01
  max_length: 512
  warmup_ratio: 0.1
  gradient_checkpointing: true
  bf16: true

data:
  datasets:
    # MMLU auxiliary train - 99k examples (100%)
    - name: "cais/mmlu"
      subset: "all"
      split: "auxiliary_train"
      weight: 1.0
      format: "mmlu"
  max_samples: 99842  # Full MMLU auxiliary_train

template:
  name: "qa"
  format: |
    Question: {question}

    Choices:
    A) {choice_a}
    B) {choice_b}
    C) {choice_c}
    D) {choice_d}

    Answer: {answer}

validation:
  split: 0.05
  eval_every: 1

logging:
  tensorboard: true
  log_every: 10
  metrics:
    - loss
    - perplexity
    - learning_rate
    - val_loss
    - val_perplexity

specialization:
  name: "mmlu_pure"
  skills:
    - factual_knowledge
    - multiple_choice
    - academic_reasoning
