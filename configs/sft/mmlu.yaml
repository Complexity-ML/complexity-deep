# MMLU SFT Configuration
# Fine-tuning on MMLU + academic QA datasets for knowledge benchmarks

model:
  checkpoint: "./checkpoints-conv-sft/checkpoint_epoch5.pt"
  tokenizer: "./checkpoints/pacific-prime-math-v2"
  output: "./checkpoints/pacific-prime-mmlu"

training:
  epochs: 15
  batch_size: 4
  gradient_accumulation: 16
  learning_rate: 1e-5
  weight_decay: 0.01
  max_length: 512
  warmup_ratio: 0.05
  gradient_checkpointing: true
  bf16: true

data:
  split: "train"  # Default split for most datasets
  datasets:
    # MMLU auxiliary train - 99k examples (boosted weight for grokking hunt)
    - name: "cais/mmlu"
      subset: "all"
      split: "auxiliary_train"  # MMLU uses this split name
      weight: 0.50
      format: "mmlu"
    # SciQ - Science questions
    - name: "allenai/sciq"
      weight: 0.12
      format: "sciq"
    # ARC - Reasoning challenges
    - name: "allenai/ai2_arc"
      subset: "ARC-Challenge"
      split: "train"
      weight: 0.08
      format: "arc"
    - name: "allenai/ai2_arc"
      subset: "ARC-Easy"
      split: "train"
      weight: 0.08
      format: "arc"
    # OpenBookQA
    - name: "allenai/openbookqa"
      split: "train"
      weight: 0.10
      format: "openbookqa"
    # HellaSwag - Sentence completion (added for transfer learning)
    - name: "Rowan/hellaswag"
      split: "train"
      weight: 0.12
      format: "hellaswag"
  max_samples: 150000

template:
  name: "qa"
  format: |
    Question: {question}

    Choices:
    A) {choice_a}
    B) {choice_b}
    C) {choice_c}
    D) {choice_d}

    Answer: {answer}

logging:
  tensorboard: true
  log_every: 10
  metrics:
    - loss
    - perplexity
    - learning_rate

specialization:
  name: "mmlu"
  skills:
    - factual_knowledge
    - multiple_choice
    - academic_reasoning
    - science
    - math
