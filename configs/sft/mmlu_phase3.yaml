# MMLU SFT Configuration - Phase 3 (epochs 21-40)
# Continue training from epoch 20 checkpoint

model:
  checkpoint: "./checkpoints-conv-sft/checkpoint_epoch15.pt"  # epoch 20 total = epoch 15 phase 2
  tokenizer: "./checkpoints-conv-sft"
  output: "./checkpoints-conv-sft"

training:
  epochs: 20
  batch_size: 8
  gradient_accumulation: 4
  learning_rate: 5e-6  # Lower LR for continued training (was 1e-5)
  weight_decay: 0.01
  max_length: 512
  warmup_ratio: 0.02  # Shorter warmup for continuation
  gradient_checkpointing: true
  bf16: true

data:
  split: "train"
  datasets:
    # MMLU auxiliary train - 99k examples
    - name: "cais/mmlu"
      subset: "all"
      split: "auxiliary_train"
      weight: 0.50
      format: "mmlu"
    # SciQ - Science questions
    - name: "allenai/sciq"
      weight: 0.12
      format: "sciq"
    # ARC - Reasoning challenges
    - name: "allenai/ai2_arc"
      subset: "ARC-Challenge"
      split: "train"
      weight: 0.08
      format: "arc"
    - name: "allenai/ai2_arc"
      subset: "ARC-Easy"
      split: "train"
      weight: 0.08
      format: "arc"
    # OpenBookQA
    - name: "allenai/openbookqa"
      split: "train"
      weight: 0.10
      format: "openbookqa"
    # HellaSwag - Sentence completion
    - name: "Rowan/hellaswag"
      split: "train"
      weight: 0.12
      format: "hellaswag"
  max_samples: 150000

template:
  name: "qa"
  format: |
    Question: {question}

    Choices:
    A) {choice_a}
    B) {choice_b}
    C) {choice_c}
    D) {choice_d}

    Answer: {answer}

logging:
  tensorboard: true
  log_every: 10
  metrics:
    - loss
    - perplexity
    - learning_rate

specialization:
  name: "mmlu_phase3"
  skills:
    - factual_knowledge
    - multiple_choice
    - academic_reasoning
    - science
    - math
